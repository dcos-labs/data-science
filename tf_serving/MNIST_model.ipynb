{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model for Tensorflow Serving\n",
    "## Lifted mostly from https://www.tensorflow.org/tfx/serving/tutorials/Serving_REST_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_images.shape: (60000, 28, 28, 1), of float64\n",
      "test_images.shape: (10000, 28, 28, 1), of float64\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# scale the values to 0.0 to 1.0\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# reshape for feeding into the model\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
    "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chris/anaconda3/envs/TFX/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 17s 287us/sample - loss: 0.5489 - acc: 0.8104\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 16s 262us/sample - loss: 0.4205 - acc: 0.8527\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 16s 267us/sample - loss: 0.3899 - acc: 0.8633\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 17s 285us/sample - loss: 0.3672 - acc: 0.8715\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 17s 277us/sample - loss: 0.3523 - acc: 0.8755\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.3879 - acc: 0.8666\n",
      "\n",
      "Test accuracy: 0.866599977016449\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
    "                      strides=2, activation='relu', name='Conv1'),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "testing = False\n",
    "epochs = 5\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_path = /tmp/mnist/1\n",
      "\n",
      "\n",
      "Already saved a model, cleaning up\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-7365875fbe01>:18: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /home/chris/anaconda3/envs/TFX/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/mnist/1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# Fetch the Keras session and save the model\n",
    "# The signature definition is defined by the input and output tensors,\n",
    "# and stored with the default serving key\n",
    "import tempfile\n",
    "\n",
    "MODEL_DIR = tempfile.gettempdir() + '/mnist'\n",
    "version = 1\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "print('export_path = {}\\n'.format(export_path))\n",
    "if os.path.isdir(export_path):\n",
    "  print('\\nAlready saved a model, cleaning up\\n')\n",
    "  !rm -r {export_path}\n",
    "\n",
    "tf.saved_model.simple_save(\n",
    "    keras.backend.get_session(),\n",
    "    export_path,\n",
    "    inputs={'input_image': model.input},\n",
    "    outputs={t.name:t for t in model.outputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "View image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb1580768d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pic = test_images[0:1]\n",
    "plt.imshow(pic.reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save image as image.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": pic.tolist()})\n",
    "with open('image.json', 'w') as f:\n",
    "  f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model to serving directory, then check<br>\n",
    "curl -X POST -H \"Content-Type: application/json\" -d @image.json http://loadbalancerIP:assginedPORT/v1/models/mnist:predict\n",
    "<br>Result:<br>\n",
    "{\n",
    "    \"predictions\": [[1.65433e-06, 1.64561e-07, 3.53323e-06, 2.86745e-06, 6.96201e-06, 0.0273952, 2.95962e-05, 0.157299, 0.00583552, 0.809425]\n",
    "    ]\n",
    "}\n",
    "<br> Therefore, the last element is most likely (Ankle boot)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFX",
   "language": "python",
   "name": "tfx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
