{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "A thorough explanation of random forests can be found [here](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier as RF\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit, udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small function to extract the probabilities from a DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "ith = udf(ith_, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Spark Context\n",
    "Get the numner of executors and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor: 6\n",
      "Node Count: 6\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "e = sc._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"Executor:\", e)\n",
    "\n",
    "s = sc._jsc.sc().getExecutorMemoryStatus().keys()\n",
    "l = str(s).replace(\"Set(\", \"\").replace(\")\", \"\").split(\", \")\n",
    "\n",
    "d = set()\n",
    "for i in l:\n",
    "    d.add(i.split(\":\")[0])\n",
    "\n",
    "print(\"Node Count:\", len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "* Load data\n",
    "* Remove useless columns and records (only CASH_OUT and TRANSFER have fraud).\n",
    "* Remap transfer to 0/1.\n",
    "\n",
    "Note: This is NOT the right way to load data into Spark. Usually, the data exists in s3, HDFS or another distributed file system. For purposes of this example it is easiest to do this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Count: 1305514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(step=2, type='PAYMENT', amount=18211.33, nameOrig='C1099717276', oldbalanceOrg=88.0, newbalanceOrig=0.0, nameDest='M417557780', oldbalanceDest=0.0, newbalanceDest=0.0, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=93240.07, nameOrig='C1350751778', oldbalanceOrg=47.0, newbalanceOrig=93287.07, nameDest='C665576141', oldbalanceDest=12.0, newbalanceDest=8650239.39, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=78314.86, nameOrig='C332699949', oldbalanceOrg=93287.07, newbalanceOrig=171601.93, nameDest='C1359044626', oldbalanceDest=178957.0, newbalanceDest=16435074.66, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=101282.39, nameOrig='C808417649', oldbalanceOrg=171601.93, newbalanceOrig=171601.93, nameDest='C1599771323', oldbalanceDest=171601.93, newbalanceDest=3771328.56, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=24227.29, nameOrig='C858204589', oldbalanceOrg=171601.93, newbalanceOrig=195829.22, nameDest='C353842779', oldbalanceDest=524999.0, newbalanceDest=872623.24, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=318148.62, nameOrig='C1976752685', oldbalanceOrg=195829.22, newbalanceOrig=513977.84, nameDest='C353842779', oldbalanceDest=500771.71, newbalanceDest=872623.24, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=140771.51, nameOrig='C1115170891', oldbalanceOrg=513977.84, newbalanceOrig=654749.35, nameDest='C1612799726', oldbalanceDest=161871.0, newbalanceDest=1297343.0, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=44248.2, nameOrig='C156710276', oldbalanceOrg=654749.35, newbalanceOrig=698997.55, nameDest='C1709537756', oldbalanceDest=83141.0, newbalanceDest=360158.53, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=72084.26, nameOrig='C53940034', oldbalanceOrg=698997.55, newbalanceOrig=771081.81, nameDest='C184966243', oldbalanceDest=73306.0, newbalanceDest=1543311.15, isFraud=0, isFlaggedFraud=0), Row(step=2, type='CASH_IN', amount=366019.81, nameOrig='C1616359099', oldbalanceOrg=771081.81, newbalanceOrig=771081.81, nameDest='C1599771323', oldbalanceDest=771081.81, newbalanceDest=3771328.56, isFraud=0, isFlaggedFraud=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call('cp ~/data-science/jupyter_home/simulated_transactions.csv.xz  /tmp/', shell=True)\n",
    "subprocess.call('xz -d /tmp/simulated_transactions.csv.xz', shell=True)\n",
    "\n",
    "df = spark.createDataFrame(pd.read_csv('/tmp/simulated_transactions.csv'))\n",
    "print(\"Record Count:\", df.count())\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "From the above we can see we read in a total of over 6 million records with 11 columns.\n",
    "<br>The description of the 11 columnes follows:\n",
    "\n",
    "|Variable|Description|Keep|\n",
    "| :------| :---------| :--|\n",
    "|step|Maps a unit of time in the real world. In this case 1 step is 1 hour of time.|Drop|\n",
    "|type|CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER|Keep (TRANSFER and CASH-OUT)|\n",
    "|amount|The amount of the transaction.|Keep|\n",
    "|nameOrig|The customer ID for the initiator of the transaction.|Drop|\n",
    "|oldbalanceOrg|The initial balance before the transaction.|Keep|\n",
    "|newbalanceOrg|The customer's balance after the transaction.|Keep|\n",
    "|nameDest|The customer ID for the recipient of the transaction.|Drop|\n",
    "|oldbalanceDest|The initial recipient balance before the transaction.|Keep|\n",
    "|newbalanceDest|The recipient's balance after the transaction.|Keep|\n",
    "|isFraud|This identifies a fraudulent transaction (1) and non fraudulent transaction(0).|Keep|\n",
    "|isFlaggedFraud|This is a rule based system that flags illegal attempts to transfer more than 200.000 in a single transaction.|Drop|\n",
    "\n",
    "### Filtering\n",
    "Filter out types other than TRANSFER, and CASH_OUT.<br>\n",
    "Remove variables, 'step', 'nameOrig', 'nameDest', and 'isFlaggedFraud'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Count: 586965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(isFraud=0, isTransfer=0, amount=85351.19, oldbalanceOrg=32935.0, newbalanceOrig=0.0, oldbalanceDest=3.0, newbalanceDest=1030012.31), Row(isFraud=0, isTransfer=0, amount=158572.35, oldbalanceOrg=57.0, newbalanceOrig=0.0, oldbalanceDest=77.0, newbalanceDest=3259761.6), Row(isFraud=0, isTransfer=0, amount=34139.45, oldbalanceOrg=30.0, newbalanceOrig=0.0, oldbalanceDest=72.0, newbalanceDest=1660399.89), Row(isFraud=0, isTransfer=0, amount=185430.62, oldbalanceOrg=7.0, newbalanceOrig=0.0, oldbalanceDest=62.0, newbalanceDest=788131.95), Row(isFraud=0, isTransfer=0, amount=288980.51, oldbalanceOrg=78.0, newbalanceOrig=0.0, oldbalanceDest=71.0, newbalanceDest=3042241.45), Row(isFraud=0, isTransfer=0, amount=85855.0, oldbalanceOrg=10.0, newbalanceOrig=0.0, oldbalanceDest=90.0, newbalanceDest=2774338.5), Row(isFraud=0, isTransfer=0, amount=127327.5, oldbalanceOrg=0.0, newbalanceOrig=0.0, oldbalanceDest=289051.51, newbalanceDest=3042241.45), Row(isFraud=0, isTransfer=0, amount=13100.27, oldbalanceOrg=0.0, newbalanceOrig=0.0, oldbalanceDest=158649.35, newbalanceDest=3259761.6), Row(isFraud=0, isTransfer=0, amount=189540.43, oldbalanceOrg=0.0, newbalanceOrig=0.0, oldbalanceDest=416379.0, newbalanceDest=3042241.45), Row(isFraud=0, isTransfer=0, amount=346689.44, oldbalanceOrg=0.0, newbalanceOrig=0.0, oldbalanceDest=605919.43, newbalanceDest=3042241.45)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"table\")\n",
    "\n",
    "df = spark.sql('''\n",
    "    SELECT isFraud,\n",
    "    CAST(type = 'TRANSFER' AS INTEGER) isTransfer,\n",
    "    amount,\n",
    "    oldbalanceOrg, newbalanceOrig,\n",
    "    oldbalanceDest, newbalanceDest\n",
    "    FROM table\n",
    "    WHERE type IN ('CASH_OUT', 'TRANSFER')\n",
    "''')\n",
    "\n",
    "print(\"Record Count:\", df.count())\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "Begin preparing for the model\n",
    "\n",
    "### Training Set\n",
    "Create a pyspark pipeline. Define features and the dependent variable. <br>\n",
    "Partition the data with an 80/20 split: Training/Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols = ['isTransfer', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "assembler_features = VectorAssembler(inputCols=featureCols, outputCol='features')\n",
    "labelIndexer = StringIndexer(inputCol='isFraud', outputCol=\"label\")\n",
    "\n",
    "dfX = [assembler_features, labelIndexer]\n",
    "pipeline = Pipeline(stages=dfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pipeline.fit(df).transform(df)\n",
    "\n",
    "trainingData, testData = allData.randomSplit([0.8, 0.2], seed=0)\n",
    "rf = RF(labelCol='label', featuresCol='features', numTrees=200)\n",
    "fit = rf.fit(trainingData)\n",
    "transformed = fit.transform(testData)\n",
    "results = transformed.select(['probability', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   label  prediction   count\n",
       "0    0.0           0  115831\n",
       "1    0.0           1       6\n",
       "2    1.0           0     156\n",
       "3    1.0           1    1512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    "\n",
    "metrics = metric(scoreAndLabels)\n",
    "validation = results.select(['label', (ith(\"probability\", lit(1)) > 0.5).cast('integer').alias('prediction') ])\n",
    "\n",
    "truth_table = validation.groupBy(['label', 'prediction']).count().orderBy(['label', 'prediction'])\n",
    "\n",
    "tt = truth_table.toPandas()\n",
    "tp = tt[((tt.label == 1) & (tt.prediction == 1))]['count'].values[0]\n",
    "fp = tt[((tt.label == 0) & (tt.prediction == 1))]['count'].values[0]\n",
    "fn = tt[((tt.label == 1) & (tt.prediction == 0))]['count'].values[0]\n",
    "tn = tt[((tt.label == 0) & (tt.prediction == 0))]['count'].values[0]\n",
    "\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of sample accuracy = 0.9986213352623292\n",
      "Out of sample precision = 0.9960474308300395\n",
      "Out of sample recall = 0.9064748201438849\n",
      "Out of sample F1 = 0.9491525423728814\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "precision = tp/(tp + fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1 = (2.0 * precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"Out of sample accuracy =\", accuracy)\n",
    "print(\"Out of sample precision =\", precision)\n",
    "print(\"Out of sample recall =\", recall)\n",
    "print(\"Out of sample F1 =\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy - Proportion of predictions that are correct. $\\frac{True Positive + True Negative}{True Positive + True Negative + False Positive + False Negative}$\n",
    "* Precision - True positive over total positive actual cases. $\\frac{True Positive}{True Positive + False Positive}$\n",
    "* Recall - True positive over total positive predicted cases. $\\frac{True Positive}{True Positive + False Negative}$\n",
    "* F1 - A balance between Precision and Recall (harmonic mean of precision and recall) $\\frac{2 * Precision * Recall}{Precision + Recall}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.6.6\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
